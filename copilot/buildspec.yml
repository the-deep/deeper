# Buildspec runs in the build stage of your pipeline.
version: 0.2

env:
  variables:
    region: us-east-1
  parameter-store:
    DOCKERHUB_USERNAME: /copilot/global/DOCKERHUB_USERNAME
    DOCKERHUB_TOKEN: /copilot/global/DOCKERHUB_TOKEN

phases:
  install:
    runtime-versions:
      docker: 19
      ruby: 2.6

    commands:
      # Enable docker buildkit
      - export DOCKER_BUILDKIT=1
      - echo "cd into $CODEBUILD_SRC_DIR"
      - cd $CODEBUILD_SRC_DIR
      # Download the copilot linux binary.
      - wget -q https://ecs-cli-v2-release.s3.amazonaws.com/copilot-linux-v1.13.0
      - mv ./copilot-linux-v1.13.0 ./copilot-linux
      - chmod +x ./copilot-linux
      - printenv DOCKERHUB_TOKEN | docker login --username ${DOCKERHUB_USERNAME} --password-stdin

  build:
    commands:  # Custom
      - echo "Cloning server"
      - export SUB_MODULES_DEP_FILE=./deploy.json
      - export DEEP_SERVER_DEPLOY=`jq -r '.server.deploy' ${SUB_MODULES_DEP_FILE}`
      - export DEEP_SERVER_REPO=`jq -r '.server.repo' ${SUB_MODULES_DEP_FILE}`
      - export DEEP_SERVER_BRANCH=`jq -r '.server.branch' ${SUB_MODULES_DEP_FILE}`
      - >
        if ! [ "${DEEP_SERVER_DEPLOY,,}" = "true" ]; then
          echo 'No deployment required!!';
          # TODO: Stop build instead of failure.
          exit 1;
        fi
      - git clone --depth 1 --branch=${DEEP_SERVER_BRANCH} ${DEEP_SERVER_REPO} ./server
      # Server
      - export DOCKER_IMAGE_SERVER=$AWS_ACCOUNT_ID.dkr.ecr.$region.amazonaws.com/deep/worker:builder
      - export DOCKER_IMAGE_SERVER_DOMAIN=$(echo $DOCKER_IMAGE_SERVER | cut -d '/' -f 1)
      - echo "[Server] Building server worker image"
      - aws ecr get-login-password --region $region | docker login --username AWS --password-stdin $DOCKER_IMAGE_SERVER_DOMAIN
      - docker pull $DOCKER_IMAGE_SERVER || true
      - docker build -t $DOCKER_IMAGE_SERVER --cache-from $DOCKER_IMAGE_SERVER ./server
      - echo "[Server] Running tests"
      - |
          docker-compose -f ./server/gh-docker-compose.yml run --rm server wait-for-it db:5432 &&
          docker-compose -f ./server/gh-docker-compose.yml run --rm server ./manage.py graphql_schema --out /ci-share/schema-latest.graphql &&
          cmp --silent ./server/schema.graphql ./server/ci-share/schema-latest.graphql || {
            echo 'The schema.graphql is not up to date with the latest changes. Please update and push latest';
            diff ./server/schema.graphql ./server/ci-share/schema-latest.graphql;
            exit 1;
          }
      - |
          docker-compose -f ./server/gh-docker-compose.yml run --rm server python3 manage.py makemigrations --check --dry-run || {
            echo 'There are some changes to be reflected in the migration. Make sure to run makemigrations';
            exit 1;
          }
      - docker-compose -f ./server/gh-docker-compose.yml run --rm server /code/scripts/run_tests.sh
      - echo "[Server] Pushing builder image"
      - aws ecr get-login-password --region $region | docker login --username AWS --password-stdin $DOCKER_IMAGE_SERVER_DOMAIN
      - docker push $DOCKER_IMAGE_SERVER

  post_build:
    commands:
      - ls -l
      - export COLOR="false"
      # First, upgrade the cloudformation stack of every environment in the pipeline.
      - pipeline=$(cat $CODEBUILD_SRC_DIR/copilot/pipeline.yml | ruby -ryaml -rjson -e 'puts JSON.pretty_generate(YAML.load(ARGF))')
      - pl_envs=$(echo $pipeline | jq -r '.stages[].name')
      - >
        for pl_env in $pl_envs; do
          ./copilot-linux env upgrade -n $pl_env;
        done;
      # Find all the local services in the workspace.
      - svc_ls_result=$(./copilot-linux svc ls --local --json)
      - svc_list=$(echo $svc_ls_result | jq '.services')
      - >
        if [ ! "$svc_list" = null ]; then
          svcs=$(echo $svc_ls_result | jq -r '.services[].name');
        fi
      # Find all the local jobs in the workspace.
      - job_ls_result=$(./copilot-linux job ls --local --json)
      - job_list=$(echo $job_ls_result | jq '.jobs')
      - >
        if [ ! "$job_list" = null ]; then
          jobs=$(echo $job_ls_result | jq -r '.jobs[].name');
        fi
      # Raise error if no services or jobs are found.
      - >
        if [ "$svc_list" = null ] && [ "$job_list" = null ]; then
          echo "No services or jobs found for the pipeline to deploy. Please create at least one service or job and push the manifest to the remote." 1>&2;
          exit 1;
        fi
      # Generate the cloudformation templates.
      # The tag is the build ID but we replaced the colon ':' with a dash '-'.
      # We truncate the tag (from the front) to 128 characters, the limit for Docker tags
      # (https://docs.docker.com/engine/reference/commandline/tag/)
      - >
        for env in $pl_envs; do
          tag=$(sed 's/:/-/g' <<<"${CODEBUILD_BUILD_ID##*:}-${env}" | rev | cut -c 1-128 | rev)
          for svc in $svcs; do
          ./copilot-linux svc package -n $svc -e $env --output-dir './infrastructure' --tag $tag;
          done;
          for job in $jobs; do
          ./copilot-linux job package -n $job -e $env --output-dir './infrastructure' --tag $tag;
          done;
        done;
      - ls -lah ./infrastructure
      # Concatenate jobs and services into one var for addons
      # If addons exists, upload addons templates to each S3 bucket and write template URL to template config files.
      - WORKLOADS=$(echo $jobs $svcs)
      - |
        for workload in $WORKLOADS; do
          ADDONSFILE=./infrastructure/$workload.addons.stack.yml
          if [ -f "$ADDONSFILE" ]; then
            tmp=$(mktemp)
            timestamp=$(date +%s)
            aws s3 cp "$ADDONSFILE" "s3://stackset-deep-infrastruc-pipelinebuiltartifactbuc-13dg2hmi292yt/manual/$timestamp/$workload.addons.stack.yml";
            jq --arg a "https://stackset-deep-infrastruc-pipelinebuiltartifactbuc-13dg2hmi292yt.s3.us-east-1.amazonaws.com/manual/$timestamp/$workload.addons.stack.yml" '.Parameters.AddonsTemplateURL = $a' ./infrastructure/$workload-staging.params.json > "$tmp" && mv "$tmp" ./infrastructure/$workload-staging.params.json
            jq --arg a "https://stackset-deep-infrastruc-pipelinebuiltartifactbuc-13dg2hmi292yt.s3.us-east-1.amazonaws.com/manual/$timestamp/$workload.addons.stack.yml" '.Parameters.AddonsTemplateURL = $a' ./infrastructure/$workload-prod.params.json > "$tmp" && mv "$tmp" ./infrastructure/$workload-prod.params.json
          fi
        done;
      # Build images
      # - For each manifest file:
      #   - Read the path to the Dockerfile by translating the YAML file into JSON.
      #   - Run docker build.
      #   - For each environment:
      #     - Retrieve the ECR repository.
      #     - Login and push the image.
      - >
        for workload in $WORKLOADS; do
          manifest=$(cat $CODEBUILD_SRC_DIR/copilot/$workload/manifest.yml | ruby -ryaml -rjson -e 'puts JSON.pretty_generate(YAML.load(ARGF))')
          for env in $pl_envs; do
            tag=$(sed 's/:/-/g' <<<"${CODEBUILD_BUILD_ID##*:}-${env}" | rev | cut -c 1-128 | rev)
            images=$(echo $manifest | jq --arg env "$env" '{base_image: .image, env_image: (.environments? | .[$env]? | .image? // {}) }')
            image=$(echo $images | jq 'if (.env_image | length == 0) then .base_image else (.base_image | del(.location)) * .env_image end')
            image_location=$(echo $image | jq '.location')
            if [ ! "$image_location" = null ]; then
              echo "skipping image building because location is provided as $image_location";
              continue
            fi
            base_dockerfile=$(echo $image | jq '.build')
            build_dockerfile=$(echo $image | jq -r '.build?.dockerfile? // ""')
            build_context=$(echo $image | jq -r '.build?.context? // ""')
            build_target=$(echo $image | jq -r '.build?.target? // ""')
            dockerfile_args=$(echo $image | jq '.build?.args? // "" | to_entries?')
            build_cache_from=$(echo $image | jq -r '.build?.cache_from? // ""')
            df_rel_path=$( echo $base_dockerfile | sed 's/"//g')
            if [ -n "$build_dockerfile" ]; then
              df_rel_path=$build_dockerfile
            fi
            df_path=$df_rel_path
            df_dir_path=$(dirname "$df_path")
            if [ -n "$build_context" ]; then
              df_dir_path=$build_context
            fi
            platform_string=$(echo $manifest | jq -r '.platform // ""')
            platform_os=$(echo $manifest | jq -r '.platform?.osfamily? // ""')
            platform_arch=$(echo $manifest | jq -r '.platform?.architecture? // ""')
            if [ "$platform_os" -a "$platform_arch" ]; then
              platform_string="$platform_os/$platform_arch"
            fi
            build_args=
            if [ -n "$dockerfile_args" ]; then
              for arg in $(echo $dockerfile_args | jq -r '.[] | "\(.key)=\(.value)"'); do
                build_args="$build_args--build-arg $arg "
              done
            fi
            if [ -n "$build_target" ]; then
              build_args="$build_args--target $build_target "
            fi
            if [ -n "$build_cache_from" ]; then
              for arg in $(echo $build_cache_from | jq -r '.[]'); do
                build_args="$build_args--cache-from $arg "
              done
            fi
            if [ -n "$platform_string" ]; then
              build_args="$build_args--platform $platform_string "
            fi
            echo "Name: $workload"
            echo "Relative Dockerfile path: $df_rel_path"
            echo "Docker build context: $df_dir_path"
            echo "Docker build args: $build_args"
            echo "Running command: docker build -t $workload:$tag $build_args-f $df_path $df_dir_path";
            docker build --cache-from $DOCKER_IMAGE_SERVER -t $workload:$tag $build_args-f $df_path $df_dir_path;
            image_id=$(docker images -q $workload:$tag);
            repo=$(cat $CODEBUILD_SRC_DIR/infrastructure/$workload-$env.params.json | jq -r '.Parameters.ContainerImage');
            region=$(echo $repo | cut -d'.' -f4);
            $(aws ecr get-login-password --region $region | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$region.amazonaws.com);
            docker tag $image_id $repo;
            docker push $repo;
          done;
        done;

artifacts:
  files:
    - "infrastructure/*"
